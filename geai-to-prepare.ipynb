{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13851020,"sourceType":"datasetVersion","datasetId":8822973}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Cell 1: Library Imports & Setup ===\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom scipy.stats import skew, kurtosis  # 新增這個！\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\n# 設定隨機種子 (為了讓結果可重現，這對比賽很重要)\nnp.random.seed(42)\n\nDATA_DIR = \"/kaggle/input/data02\" # 記得確認你的路徑是否正確\n\n# 讀取 Log 檔\ntrain_log = pd.read_csv(f\"{DATA_DIR}/train_log.csv\")\ntest_log  = pd.read_csv(f\"{DATA_DIR}/test_log.csv\")\n\nprint(\"Libraries loaded & Random Seed Fixed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:41:01.475952Z","iopub.execute_input":"2025-11-24T14:41:01.476270Z","iopub.status.idle":"2025-11-24T14:41:01.516761Z","shell.execute_reply.started":"2025-11-24T14:41:01.476247Z","shell.execute_reply":"2025-11-24T14:41:01.515729Z"}},"outputs":[{"name":"stdout","text":"Libraries loaded & Random Seed Fixed.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# === Cell 2: Advanced Feature Engineering ===\n\ndef calculate_eta(x):\n    \"\"\"\n    計算 von Neumann Ratio (Eta)。\n    數值越小 = 軌跡越平滑 (規律)；數值越大 = 越像雜訊。\n    \"\"\"\n    if len(x) < 2:\n        return 0.0\n    sigma2 = np.var(x, ddof=1)\n    if sigma2 == 0:\n        return 0.0\n    diff = np.diff(x)\n    return np.mean(diff**2) / sigma2\n\ndef build_features_from_lightcurves(df):\n    # 1. 預處理：確保按時間排序\n    df = df.sort_values([\"object_id\", \"Filter\", \"Time (MJD)\"])\n    \n    # 建立加權平均用的權重 (Weight = 1 / error^2)\n    df['w'] = 1.0 / (df['Flux_err']**2 + 1e-9)\n    df['flux_w'] = df['Flux'] * df['w']\n\n    # 2. 向量化聚合 (Aggregation)\n    # 加入 skew (偏度) 和 kurtosis (峰度)\n    aggs = {\n        'Flux': ['min', 'max', 'mean', 'median', 'std', skew, kurtosis], \n        'Flux_err': ['mean'],\n        'Time (MJD)': ['min', 'max', 'count'], \n        'w': ['sum'],\n        'flux_w': ['sum']\n    }\n    \n    # 執行 Groupby 計算\n    agg_df = df.groupby(['object_id', 'Filter']).agg(aggs)\n    agg_df.columns = [f\"{k}_{v}\" for k, v in agg_df.columns]\n    agg_df = agg_df.reset_index()\n    \n    # 3. 計算聚合後的基礎特徵\n    agg_df['flux_amp'] = agg_df['Flux_max'] - agg_df['Flux_min'] # 振幅\n    agg_df['flux_rel_amp'] = agg_df['flux_amp'] / (agg_df['Flux_mean'] + 1e-9) # 相對振幅\n    agg_df['time_span'] = agg_df['Time (MJD)_max'] - agg_df['Time (MJD)_min'] # 觀測時間跨度\n    \n    # 計算加權平均亮度 (比普通平均更準)\n    agg_df['flux_w_mean'] = agg_df['flux_w_sum'] / (agg_df['w_sum'] + 1e-9)\n    \n    # 4. 計算變異性指標 (Eta) - 使用 apply\n    # 這是天文學判斷變星的重要指標\n    eta_df = df.groupby(['object_id', 'Filter'])['Flux'].apply(calculate_eta).reset_index()\n    eta_df.columns = ['object_id', 'Filter', 'flux_eta']\n    \n    # 合併 Eta\n    agg_df = agg_df.merge(eta_df, on=['object_id', 'Filter'], how='left')\n    \n    # 清理中間變數以節省記憶體\n    agg_df.drop(columns=['w_sum', 'flux_w_sum'], inplace=True)\n\n    # 5. Pivot (轉置): 將 Filter 0, 1... 轉成欄位\n    agg_df['has_filter'] = 1\n    \n    # Filter 存在性特徵\n    filter_presence = agg_df.pivot(index=\"object_id\", columns=\"Filter\", values=\"has_filter\").fillna(0)\n    filter_presence.columns = [f\"has_{c}\" for c in filter_presence.columns]\n    \n    # 數值特徵轉置\n    numeric_cols = [c for c in agg_df.columns if c not in ['object_id', 'Filter', 'has_filter']]\n    feats_wide = agg_df.pivot(index=\"object_id\", columns=\"Filter\", values=numeric_cols)\n    feats_wide.columns = [f\"{c[0]}_{c[1]}\" for c in feats_wide.columns] # e.g., Flux_mean_0\n    \n    feats_wide = feats_wide.reset_index().fillna(0.0)\n    filter_presence = filter_presence.reset_index()\n\n    final_df = feats_wide.merge(filter_presence, on=\"object_id\", how=\"left\")\n    \n    # ==========================================\n    # 6. 新增：Color Features (顏色特徵 - 物理核心)\n    # ==========================================\n    # 自動偵測有哪些濾鏡，並計算它們之間的亮度差 (顏色)\n    available_filters = sorted(df['Filter'].unique())\n    \n    for i in range(len(available_filters)):\n        for j in range(i + 1, len(available_filters)):\n            f1 = available_filters[i]\n            f2 = available_filters[j]\n            \n            # 使用 Weighted Mean 來算顏色\n            col1 = f\"flux_w_mean_{f1}\"\n            col2 = f\"flux_w_mean_{f2}\"\n            \n            if col1 in final_df.columns and col2 in final_df.columns:\n                # 顏色 (亮度差) -> 代表溫度\n                final_df[f\"color_{f1}_{f2}\"] = final_df[col1] - final_df[col2]\n                # 顏色比例\n                final_df[f\"color_ratio_{f1}_{f2}\"] = final_df[col1] / (final_df[col2] + 1e-6)\n\n    return final_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:41:01.518541Z","iopub.execute_input":"2025-11-24T14:41:01.518898Z","iopub.status.idle":"2025-11-24T14:41:01.534914Z","shell.execute_reply.started":"2025-11-24T14:41:01.518870Z","shell.execute_reply":"2025-11-24T14:41:01.533718Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# === Cell 3: Data Loading & Physics Interactions ===\n\ndef load_and_process_splits(split_ids, mode=\"train\"):\n    all_feats = []\n    \n    for i in split_ids:\n        fname = f\"{DATA_DIR}/split_{i:02d}/{mode}_full_lightcurves.csv\"\n        if os.path.exists(fname):\n            print(f\"Processing {fname}...\")\n            df = pd.read_csv(fname)\n            feats = build_features_from_lightcurves(df)\n            all_feats.append(feats)\n            \n            # 記憶體管理\n            del df\n            gc.collect()\n            \n    full_df = pd.concat(all_feats, ignore_index=True)\n    \n    # 處理重複的 object_id (取平均)\n    if full_df.duplicated(subset=['object_id']).any():\n        full_df = full_df.groupby(\"object_id\", as_index=False).mean()\n        \n    return full_df\n\n# 讀取 Train (這一步會跑比較久，因為特徵變多了)\nprint(\"Loading TRAIN data with advanced features...\")\ntrain_feats = load_and_process_splits(range(1, 21), mode=\"train\")\n\n# 合併 Meta Data\ntrain = train_feats.merge(\n    train_log[[\"object_id\",\"Z\",\"Z_err\",\"EBV\",\"target\"]],\n    on=\"object_id\",\n    how=\"left\"\n).fillna(0)\n\n# ==========================================\n# Level 2 特徵工程：物理交互項\n# ==========================================\n\n# 1. 亮度與距離(Z)的交互作用 -> 模擬「絕對星等」\nfor col in train.columns:\n    if \"flux_w_mean\" in col: # 針對所有濾鏡的平均亮度\n        train[col + \"_x_Z\"] = train[col] * train[\"Z\"]\n        # train[col + \"_x_Z_sq\"] = train[col] * (train[\"Z\"]**2) # 可選：距離平方反比\n\n# 2. 顏色與 EBV (星際消光) 的修正\n# EBV 會讓星星變紅，我們要扣掉這個影響\nfor col in train.columns:\n    if \"color_\" in col and \"ratio\" not in col:\n        train[col + \"_minus_EBV\"] = train[col] - train[\"EBV\"]\n\nprint(\"Train shape:\", train.shape)\nprint(\"Ready for training!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:41:01.535994Z","iopub.execute_input":"2025-11-24T14:41:01.536369Z","iopub.status.idle":"2025-11-24T14:41:31.873285Z","shell.execute_reply.started":"2025-11-24T14:41:01.536346Z","shell.execute_reply":"2025-11-24T14:41:31.872197Z"}},"outputs":[{"name":"stdout","text":"Loading TRAIN data with advanced features...\nProcessing /kaggle/input/data02/split_01/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_02/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_03/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_04/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_05/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_06/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_07/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_08/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_09/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_10/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_11/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_12/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_13/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_14/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_15/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_16/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_17/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_18/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_19/train_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_20/train_full_lightcurves.csv...\nTrain shape: (3043, 158)\nReady for training!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"feature_cols = [c for c in train.columns if c not in [\"object_id\", \"target\"]]\nX = train[feature_cols].values\ny = train[\"target\"].values\n\n# 設定 K-Fold\nN_FOLDS = 5\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n\n# 儲存預測結果\noof_preds = np.zeros(X.shape[0])\nmodels_lgb = []\nmodels_xgb = []\n\n# 計算權重\npos = (y == 1).sum()\nneg = (y == 0).sum()\nscale_pos_weight = neg / (pos + 1)\n\n# 參數設定\nparams_lgb = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 64, # 稍微調小避免過擬合\n    \"feature_fraction\": 0.7,\n    \"bagging_fraction\": 0.7,\n    \"bagging_freq\": 1,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"verbosity\": -1,\n}\n\nparams_xgb = {\n    \"max_depth\": 6,\n    \"eta\": 0.03,\n    \"subsample\": 0.7,\n    \"colsample_bytree\": 0.7,\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"logloss\",\n    \"scale_pos_weight\": scale_pos_weight,\n    \"tree_method\": \"hist\" # 加速\n}\n\nprint(f\"Starting {N_FOLDS}-Fold Cross Validation...\")\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"=== FOLD {fold+1}/{N_FOLDS} ===\")\n    \n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    \n    # --- LightGBM ---\n    dtrain_lgb = lgb.Dataset(X_train, label=y_train)\n    dval_lgb = lgb.Dataset(X_val, label=y_val)\n    \n    clf_lgb = lgb.train(\n        params_lgb,\n        dtrain_lgb,\n        num_boost_round=1000,\n        valid_sets=[dtrain_lgb, dval_lgb],\n        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)] # 靜默模式\n    )\n    models_lgb.append(clf_lgb)\n    val_pred_lgb = clf_lgb.predict(X_val)\n    \n    # --- XGBoost ---\n    dtrain_xgb = xgb.DMatrix(X_train, label=y_train)\n    dval_xgb = xgb.DMatrix(X_val, label=y_val)\n    \n    clf_xgb = xgb.train(\n        params_xgb,\n        dtrain_xgb,\n        num_boost_round=1000,\n        evals=[(dval_xgb, \"eval\")],\n        early_stopping_rounds=50,\n        verbose_eval=False\n    )\n    models_xgb.append(clf_xgb)\n    val_pred_xgb = clf_xgb.predict(dval_xgb)\n    \n    # Ensemble OOF\n    oof_preds[val_idx] = (val_pred_lgb + val_pred_xgb) / 2\n\n# 尋找最佳閾值 (基於 OOF)\nbest_thr, best_f1 = 0, 0\nfor thr in np.linspace(0.05, 0.95, 100):\n    pred_label = (oof_preds >= thr).astype(int)\n    f1 = f1_score(y, pred_label)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thr = thr\n\nprint(f\"\\nCV F1 Score: {best_f1:.4f}\")\nprint(f\"Best Threshold: {best_thr:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:41:31.874519Z","iopub.execute_input":"2025-11-24T14:41:31.874776Z","iopub.status.idle":"2025-11-24T14:41:58.873582Z","shell.execute_reply.started":"2025-11-24T14:41:31.874749Z","shell.execute_reply":"2025-11-24T14:41:58.872651Z"}},"outputs":[{"name":"stdout","text":"Starting 5-Fold Cross Validation...\n=== FOLD 1/5 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[70]\ttraining's binary_logloss: 0.0436114\tvalid_1's binary_logloss: 0.157195\n=== FOLD 2/5 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[99]\ttraining's binary_logloss: 0.0278837\tvalid_1's binary_logloss: 0.146123\n=== FOLD 3/5 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[52]\ttraining's binary_logloss: 0.0561431\tvalid_1's binary_logloss: 0.152988\n=== FOLD 4/5 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[91]\ttraining's binary_logloss: 0.0315416\tvalid_1's binary_logloss: 0.137688\n=== FOLD 5/5 ===\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[106]\ttraining's binary_logloss: 0.025914\tvalid_1's binary_logloss: 0.122066\n\nCV F1 Score: 0.3981\nBest Threshold: 0.159\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# 讀取 Test (使用與 Train 相同的邏輯)\ntest_feats = load_and_process_splits(range(1, 21), mode=\"test\")\n\ntest = test_feats.merge(\n    test_log[[\"object_id\",\"Z\",\"Z_err\",\"EBV\"]],\n    on=\"object_id\",\n    how=\"left\"\n).fillna(0)\n\n# Test Z Interaction\ninteract_bases = [\"Flux_mean\", \"Flux_std\", \"flux_amp\", \"flux_slope\", \"flux_w_mean\"]\nfor base in interact_bases:\n    cols = [c for c in test.columns if base in c]\n    for col in cols:\n        test[col + \"_Z\"] = test[col] * test[\"Z\"]\n\n# 確保欄位一致\nX_test = test[feature_cols].values\ndtest = xgb.DMatrix(X_test)\n\n# 推論 (對所有 Fold 取平均)\nsub_preds = np.zeros(X_test.shape[0])\n\nprint(\"Predicting on test set...\")\nfor i in range(N_FOLDS):\n    pred_lgb = models_lgb[i].predict(X_test)\n    pred_xgb = models_xgb[i].predict(dtest)\n    sub_preds += (pred_lgb + pred_xgb) / 2\n\nsub_preds /= N_FOLDS\nprint(\"Prediction complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:41:58.875526Z","iopub.execute_input":"2025-11-24T14:41:58.875827Z","iopub.status.idle":"2025-11-24T14:43:04.875673Z","shell.execute_reply.started":"2025-11-24T14:41:58.875778Z","shell.execute_reply":"2025-11-24T14:43:04.874228Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/data02/split_01/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_02/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_03/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_04/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_05/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_06/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_07/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_08/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_09/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_10/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_11/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_12/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_13/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_14/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_15/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_16/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_17/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_18/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_19/test_full_lightcurves.csv...\nProcessing /kaggle/input/data02/split_20/test_full_lightcurves.csv...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1689823101.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 確保欄位一致\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['flux_w_mean_g_x_Z', 'flux_w_mean_i_x_Z', 'flux_w_mean_r_x_Z', 'flux_w_mean_u_x_Z', 'flux_w_mean_y_x_Z', 'flux_w_mean_z_x_Z', 'color_g_i_minus_EBV', 'color_g_r_minus_EBV', 'color_g_u_minus_EBV', 'color_g_y_minus_EBV', 'color_g_z_minus_EBV', 'color_i_r_minus_EBV', 'color_i_u_minus_EBV', 'color_i_y_minus_EBV', 'color_i_z_minus_EBV', 'color_r_u_minus_EBV', 'color_r_y_minus_EBV', 'color_r_z_minus_EBV', 'color_u_y_minus_EBV', 'color_u_z_minus_EBV', 'color_y_z_minus_EBV'] not in index\""],"ename":"KeyError","evalue":"\"['flux_w_mean_g_x_Z', 'flux_w_mean_i_x_Z', 'flux_w_mean_r_x_Z', 'flux_w_mean_u_x_Z', 'flux_w_mean_y_x_Z', 'flux_w_mean_z_x_Z', 'color_g_i_minus_EBV', 'color_g_r_minus_EBV', 'color_g_u_minus_EBV', 'color_g_y_minus_EBV', 'color_g_z_minus_EBV', 'color_i_r_minus_EBV', 'color_i_u_minus_EBV', 'color_i_y_minus_EBV', 'color_i_z_minus_EBV', 'color_r_u_minus_EBV', 'color_r_y_minus_EBV', 'color_r_z_minus_EBV', 'color_u_y_minus_EBV', 'color_u_z_minus_EBV', 'color_y_z_minus_EBV'] not in index\"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"# ===========================\n# 修改版 Cell 6：手動測試更低的閾值\n# ===========================\n\n# 因為 0.365 分數變高，我們繼續往下降來找最高分\n# 之前的最佳紀錄是 0.14 (舊模型)，新模型可能落在 0.2~0.3 之間\nthreshold_list = [\n    0.20,\n    0.24,\n    0.28,\n    0.32,\n    0.35  # 接續剛剛的 0.365\n]\n\nprint(\"Generating lower threshold submissions:\", threshold_list)\n\nfor idx, thr in enumerate(threshold_list):\n    # 使用記憶體中已經算好的 sub_preds\n    pred_sub = (sub_preds >= thr).astype(int)\n    \n    sub_df = pd.DataFrame({\n        \"object_id\": test[\"object_id\"],\n        \"prediction\": pred_sub\n    })\n    \n    # 檔名加上 low_thr 標記以便區分\n    fname = f\"submission_low_thr{idx+1}_{thr:.2f}.csv\"\n    sub_df.to_csv(fname, index=False)\n    \n    print(f\"Saved: {fname} (Positive count: {pred_sub.sum()})\")\n\nprint(\"\\nNew low-threshold files generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T14:43:04.876489Z","iopub.status.idle":"2025-11-24T14:43:04.876848Z","shell.execute_reply.started":"2025-11-24T14:43:04.876650Z","shell.execute_reply":"2025-11-24T14:43:04.876664Z"}},"outputs":[],"execution_count":null}]}